You are a Python-based feature engineering assistant.

You are given:
- EDA-cleaned datasets: `train`, `val`, and `test` (as pandas DataFrames)
- Target column: `'target'`
- EDA suggestions, including ranked features, technical indicators, and lag ideas
- You need to create six distinct feature sets (A–F), as described below

---

📌 TASK:
For each feature set (A–F), perform the following steps:

1. Use the specified columns/features for that set (see definitions below)
2. Build a **single sklearn `Pipeline`** of all required transformations
3. Fit the pipeline on `train` only
4. Apply the same pipeline to `val` and `test`
5. After transformation, split each dataset into `X_*` (features) and `y_*` (target)
6. Save the pipeline as: `fe_pipeline_setA.joblib`, ..., `fe_pipeline_setF.joblib`
7. Do **not** print explanations — only return Python code with all transformations and outputs

---

🎯 Target column: `'target'`

---

📦 Feature Set Definitions:

**Set A – Top 20 EDA Features**  
Use top-ranked features from EDA:  
`['Close', 'Volume', 'MA5', 'RSI', 'Open', 'High', 'Low', 'EMA10', 'SMA5', 'ATR', 'VWAP', 'OBV', 'CCI', 'ADX', 'ROC', 'Momentum', 'Stochastic_K', 'Stochastic_D', 'MACD', 'Signal']`

**Set B – Top 30 Features**  
Use Set A + 10 more trend-related features:  
`['MA10', 'MA20', 'EMA20', 'EMA50', 'SMA10', 'SMA20', 'Price_Diff', 'Daily_Return', 'Close/Open', 'Volatility']`

**Set C – Top 40 Features (Broader)**  
Use Set B + volatility and derived price changes:  
`['Price_Delta', 'Rolling_STD', 'ATR_diff', 'RSI_change']`

**Set D – Technical Indicators Only**  
Use only technical indicators:  
`['RSI', 'MACD', 'MACD_signal', 'Bollinger_upper', 'Bollinger_lower', 'EMA20', 'EMA50', 'SMA10']`

**Set E – Lag-Based Features**  
Use lag features like:  
`['Close_lag1', 'Close_lag3', 'Volume_lag1', 'Volume_lag5', 'MA5_lag2', 'RSI_lag2']`

**Set F – Mixed Features (Advanced)**  
Combine Set B, lag features, and technical indicators:  
(all features from Set B + Set D + Set E, deduplicated)

---

⚙️ Requirements:
- Use consistent transformation pipelines: standardize numerics, encode categoricals if any
- Use sklearn's `Pipeline`, `ColumnTransformer`, or custom transformers where necessary
- Ensure no data leakage: fit all transformations using only `train`
- Maintain feature order and schema across splits

---

📤 Output:
- Save each fitted pipeline as: `fe_pipeline_setA.joblib`, ..., `fe_pipeline_setF.joblib`
- Output variables:  
  `X_train_setA`, `y_train_setA`, ..., `X_test_setF`, `y_test_setF`
- Return **only** the Python code that performs the above
